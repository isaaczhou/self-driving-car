#+TITLE: Self-Driving Car Nanodegree
#+OPTIONS: toc:3

* Computer Vision and Deep Learning
** Computer Vision Fundamentals
*** Color Selection Code Example
    - Coding up a Color Selection
    - Let’s code up a simple color selection in Python. No need to download or install anything, you can just follow along in the browser for now. We'll be working with the same image you saw previously.
**** Code
***** Load modules
      - First, I import pyplot and image from matplotlib. I also import numpy for operating on the image.
 #+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
#plt.style.use("fivethirtyeight")
%matplotlib inline

import matplotlib.image as mpimg
import numpy as np
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[206]:
 :END:

***** Read in image
      - I then read in an image and print out some stats. I’ll grab the x and y sizes and make a copy of the image to work with. NOTE: Always make a copy of arrays or other variables in Python. If instead, you say "a = b" then all changes you make to "a" will be reflected in "b" as well!
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
image_path = "/home/isaac/Dropbox/udacity/data/test.jpg"
image = mpimg.imread(image_path)
("This image is {a} with dimensions {b}".format(a = type(image), b=image.shape))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[207]:
: "This image is <class 'numpy.ndarray'> with dimensions (540, 960, 3)"
:END:

***** Grab the x and y
      - Grab the x and y size and make a copy of the image
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
ysize = image.shape[0]
xsize = image.shape[1]
color_select = np.copy(image)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[215]:
:END:

***** Set RGB threshold
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
red_threshold = 220
green_threshold = 220
blue_threshold = 220
rgb_threshold = [red_threshold, green_threshold, blue_threshold]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[216]:
:END:

***** Color Retrain
      - Next, I'll select any pixels below the threshold and set them to zero. After that, all pixels that meet my color criterion (those above the threshold) will be retained, and those that do not (below the threshold) will be blacked out.
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
# Identify pixels below the threshold
thresholds = (image[:,:,0] < rgb_threshold[0]) | (image[:,:,1] < rgb_threshold[1]) | (image[:,:,2] < rgb_threshold[2])
color_select[thresholds] = [0,0,0]
plt.imshow(color_select);
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[217]:
[[file:./obipy-resources/227703E.png]]
:END:


**** Review
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
%matplotlib inline

img_path = "/home/isaac/Dropbox/udacity/data/test.jpg"
image = mpimg.imread(img_path)
plt.imshow(image);
"The image is of type {a} with a shape of {b}".format(a = type(image), b = image.shape)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[526]:
: "The image is of type <class 'numpy.ndarray'> with a shape of (540, 960, 3)"
[[file:./obipy-resources/17361lPv.png]]
:END:

*** Color Selections
    - Here's how I did it… I started by just trying some guesses.
    - Eventually, I found that with red_threshold = green_threshold = blue_threshold = 200, I get a pretty good result, where I can clearly see the lane lines, but most everything else is blacked out.
    - At this point, however, it would still be tricky to extract the exact lines automatically, because we still have many other pixels detected around the periphery.

**** Review
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
%matplotlib inline

img_path = "/home/isaac/Dropbox/udacity/data/test.jpg"
image = mpimg.imread(img_path)
"The image is of type {a} with a shape of {b}".format(a = type(image), b = image.shape)

r_threshold = 200
g_threshold = 200
b_threshold = 200
image_color = np.copy(image)
image_threshold = (image[:,:,0] < r_threshold) | (image[:,:,1] < g_threshold) | (image[:,:,2] < b_threshold)
image_color[image_threshold] = [0,0,0]
plt.imshow(image_color);
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[548]:
[[file:./obipy-resources/17361YaF.png]]
:END:

*** Region Masking
    - At this point, however, it would still be tricky to extract the exact lines automatically, because we still have some other objects detected around the periphery that aren't lane lines.
    - In this case, I'll assume that the front facing camera that took the image is mounted in a fixed position on the car, such that the lane lines will always appear in the same general region of the image. Next, I'll take advantage of this by adding a criterion to only consider pixels for color selection in the region where we expect to find the lane lines.
    - Check out the code below. The variables left_bottom, right_bottom, and apex represent the vertices of a triangular region that I would like to retain for my color selection, while masking everything else out. Here I'm using a triangular mask to illustrate the simplest case, but later you'll use a quadrilateral, and in principle, you could use any polygon.

**** Code
     
***** Load Modules and Read in Images
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
%matplotlib inline

img_path = "/home/isaac/Dropbox/udacity/data/test.jpg"
image = mpimg.imread(img_path)
"This image is {a} with {b} dimensions".format(a = type(image), b=image.shape)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[218]:
: "This image is <class 'numpy.ndarray'> with (540, 960, 3) dimensions"
:END:

***** Pull out X,Y
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
ysize = image.shape[0]
xsize = image.shape[1]
region_select = np.copy(image)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[322]:
:END:

***** Define a Triangle Region
      - Define a triangle region of interest
      - Keep in mind the origin (x=0, y=0) is in the upper left in image processing
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
left_bottom = [150, 550]
right_bottom = [800, 550]
apex = [450, 320]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[323]:
:END:

***** Fit Lines
      - fit lines y = Ax + B to identify 3 sided region of interest
      - np.polyfit() returns the coefficients [A, B] of the fit
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]),deg=1)
fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]),deg=1)
fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]),deg=1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[324]:
:END:

***** Find the Region inside the lines
      - Find the region inside the linesysize
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))
region_threshold = (YY > (XX * fit_left[0] + fit_left[1])) & (YY > (XX * fit_right[0] + fit_right[1])) & (YY < (XX * fit_bottom[0] + fit_bottom[1]))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[325]:
:END:

***** Color pixels red which are inside the region of interest
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
region_select[region_threshold] = (255, 0, 0)
plt.imshow(region_select)
#+END_SRC

**** Review
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
%matplotlib inline

img_path = "/home/isaac/Dropbox/udacity/data/exit-ramp.jpg"
image = mpimg.imread(img_path)
"The loaded image is of type {a} and the shape is {b}".format(a = type(image), b = image.shape)
x_size, y_size = image.shape[1], image.shape[0]

left_bottom = [100, 539]
right_bottom = [860, 550]
apex = [475, 280]

fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)
fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)
fit_bottom = np.polyfit((right_bottom[0], left_bottom[0]), (right_bottom[1], left_bottom[1]), 1)

XX, YY = np.meshgrid(np.arange(0, x_size), np.arange(0, y_size))

region_thresholds = (YY > (XX * fit_left[0] + fit_left[1])) & (YY > (XX * fit_right[0] + fit_right[1])) & (YY < XX * fit_bottom[0] + fit_bottom[1])

region_select = np.copy(image)
region_select[region_thresholds] = [255,0,0]
plt.imshow(region_select, cmap="Greys_r");
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[668]:
[[file:./obipy-resources/17361brC.png]]
:END:

**** 
*** Combining Color and Region Selections
    - Now you've seen how to mask out a region of interest in an image. Next, let's combine the mask and color selection to pull only the lane lines out of the image.
    - Check out the code below. Here we’re doing both the color and region selection steps, requiring that a pixel meet both the mask and color selection requirements to be retained.

#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np

# Read in the image
img_path = "/home/isaac/Dropbox/udacity/data/test.jpg"
image = mpimg.imread(img_path)

# Grab the x and y sizes and make two copies of the image
# With one copy we'll extract only the pixels that meet our selection,
# then we'll paint those pixels red in the original image to see our selection 
# overlaid on the original.
ysize = image.shape[0]
xsize = image.shape[1]
color_select= np.copy(image)
line_image = np.copy(image)

# Define our color criteria
red_threshold = 200
green_threshold = 200
blue_threshold = 200
rgb_threshold = [red_threshold, green_threshold, blue_threshold]

# Define a triangle region of interest (Note: if you run this code, 
# Keep in mind the origin (x=0, y=0) is in the upper left in image processing
# you'll find these are not sensible values!!
# But you'll get a chance to play with them soon in a quiz ;)
left_bottom = [120, 550]
right_bottom = [800, 550]
apex = [480, 330]

fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)
fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)
fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)

# Mask pixels below the threshold
color_thresholds = (image[:,:,0] < rgb_threshold[0]) | \
                    (image[:,:,1] < rgb_threshold[1]) | \
                    (image[:,:,2] < rgb_threshold[2])

# Find the region inside the lines
XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))
region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \
                    (YY > (XX*fit_right[0] + fit_right[1])) & \
                    (YY < (XX*fit_bottom[0] + fit_bottom[1]))
# Mask color selection
color_select[color_thresholds] = [0, 0, 0]
# Find where image is both colored right and in the region
line_image[~color_thresholds & region_thresholds] = [255,0,0]

# Display our two output images
plt.imshow(color_select)
plt.imshow(line_image)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[708]:
: <matplotlib.image.AxesImage at 0x7f82a9e7ad30>
[[file:./obipy-resources/17361q3W.png]]
:END:

*** Finding lines of any color
    - As it happens, lane lines are not always the same color, and even lines of the same color under different lighting conditions (day, night, etc) may fail to be detected by our simple color selection.
    - What we need is to take our algorithm to the next level to detect lines of any color using sophisticated computer vision methods.

*** Computer Vision
    - In rest of this lesson, we’ll introduce some computer vision techniques with enough detail for you to get an intuitive feel for how they work.
    - Throughout this Nanodegree Program, we will be using Python with OpenCV for computer vision work. OpenCV stands for Open-Source Computer Vision. For now, you don't need to download or install anything, but later in the program we'll help you get these tools installed on your own computer.
    - OpenCV contains extensive libraries of functions that you can use. The OpenCV libraries are well documented, so if you’re ever feeling confused about what the parameters in a particular function are doing, or anything else, you can find a wealth of information at opencv.org.

*** Canny Edge Detection
    - the goal is to identify the boundaries of an object in an image
    - Gradient: brightness of each point corresponds to the strength of the gradient at that point
    - in opencv: edges = cv2(image, low_threshold, high_threshold)
    - rapid changes in brightness is where we find the edge
    - image is just a mathematical function image = f(x,y), we can apply all kinds of mathematical operations on the image, e.g. derivatives, which measure the change of the function.

*** Canny to Detect Lane Lines
    - Now that you have a conceptual grasp on how the Canny algorithm works, it's time to use it to find the edges of the lane lines in an image of the road. So let's give that a try.
**** Code
***** First Read Images
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
%matplotlib inline
img_path = "/home/isaac/Dropbox/udacity/data/exit-ramp.jpg"
image = mpimg.imread(img_path)
plt.imshow(image);
"The image is {a} with a dimension of {b}".format(a = type(image), b=image.shape)
#+END_SRC     

#+RESULTS:
:RESULTS:
# Out[40]:
: "The image is <class 'numpy.ndarray'> with a dimension of (540, 960, 3)"
[[file:./obipy-resources/30073SC0.png]]
:END:

***** OpenCV
      - Convert to grayscale
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import cv2
gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
plt.imshow(gray)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[43]:
: <matplotlib.image.AxesImage at 0x7fc05901db38>
[[file:./obipy-resources/30073RWJ.png]]
:END:

***** Canny edge detector
      - Let’s try our Canny edge detector on this image. This is where OpenCV gets useful. First, we'll have a look at the parameters for the OpenCV Canny function. You will call it like this: edges = cv2.Canny(gray, low_threshold, high_threshold)
      - In this case, you are applying Canny to the image gray and your output will be another image called edges. low_threshold and high_threshold are your thresholds for edge detection.
      - The algorithm will first detect strong edge (strong gradient) pixels above the high_threshold, and reject pixels below the low_threshold. Next, pixels with values between the low_threshold and high_threshold will be included as long as they are connected to strong edges. The output edges is a binary image with white pixels tracing out the detected edges and black everywhere else. See the OpenCV Canny Docs for more details.
      - What would make sense as a reasonable range for these parameters? In our case, converting to grayscale has left us with an 8-bit image, so each pixel can take 2^8 = 256 possible values. Hence, the pixel values range from 0 to 255.
      - This range implies that derivatives (essentially, the value differences from pixel to pixel) will be on the scale of tens or hundreds. So, a reasonable range for your threshold parameters would also be in the tens to hundreds.
      - As far as a ratio of low_threshold to high_threshold, John Canny himself recommended a low to high ratio of 1:2 or 1:3.
      - We'll also include Gaussian smoothing, before running Canny, which is essentially a way of suppressing noise and spurious gradients by averaging (check out the OpenCV docs for GaussianBlur). cv2.Canny() actually applies Gaussian smoothing internally, but we include it here because you can get a different result by applying further smoothing (and it's not a changeable parameter within cv2.Canny()!).
      - You can choose the kernel_size for Gaussian smoothing to be any odd number. A larger kernel_size implies averaging, or smoothing, over a larger area.
      - 

#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import cv2

img_path = "/home/isaac/Dropbox/udacity/data/exit-ramp.jpg"
image = mpimg.imread(img_path)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

kernel_size=3
blur_gray = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)

low_threshold = 100
high_threshold = 200
edges = cv2.Canny(blur_gray, low_threshold, high_threshold)

plt.imshow(edges, cmap="Greys_r");
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[522]:
[[file:./obipy-resources/17361L7i.png]]
:END:

***** Synthesis
      - Here I've called the OpenCV function Canny on a Gaussian-smoothed grayscaled image called blur_gray and detected edges with thresholds on the gradient of high_threshold, and low_threshold.

**** Review
***** Code
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import cv2
%matplotlib inline

img_path = "/home/isaac/Dropbox/udacity/data/exit-ramp.jpg"
image = mpimg.imread(img_path)
gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
gaussian_blur = cv2.GaussianBlur(gray_image, (3,3), 0)
low_threshold = 1
high_threshold = 100
edges = cv2.Canny(gaussian_blur, low_threshold, high_threshold)

plt.imshow(edges, cmap="gray");
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[89]:
[[file:./obipy-resources/305_q2.png]]
:END:
***** TODO Check [[https://www.udacity.com/course/introduction-to-computer-vision--ud810][Intro to Computer]] Vision: Lesson3
***** TODO Check [[https://www.udacity.com/course/introduction-to-computer-vision--ud810][Intro to Computer]] Vision: Lesson6 and Lesson7
*** Hough Transform
    - In image space, a line is plotted as x vs. y, but in 1962, Paul Hough devised a method for representing lines in parameter space, which we will call “Hough space” in his honor.
    - In Hough space, I can represent my "x vs. y" line as a point in "m vs. b" instead. The Hough Transform is just the conversion from image space to Hough space. So, the characterization of a line in image space will be a single point at the position (m, b) in Hough space.
    - Alright, so a line in image space corresponds to a point in Hough space. What does a point in image space correspond to in Hough space?
    - A single point in image space has many possible lines that pass through it, but not just any lines, only those with particular combinations of the m and b parameters. Rearranging the equation of a line, we find that a single point (x,y) corresponds to the line b = y - xm.
    - What if you have 2 points in image space. What would that look like in Hough space? Two intersected lines
    - Alright, now we have two intersecting lines in Hough Space. How would you represent their intersection at the point (m0, b0) in image space?

*** Hough Transform to find lane lines

**** Implementing a Hough Transform on Edge Detected Image
     - Now you know how the Hough Transform works, but to accomplish the task of finding lane lines, we need to specify some parameters to say what kind of lines we want to detect (i.e., long lines, short lines, bendy lines, dashed lines, etc.).
     - To do this, we'll be using an OpenCV function called HoughLinesP that takes several parameters. Let's code it up and find the lane lines in the image we detected edges in with the Canny function (for a look at coding up a Hough Transform from scratch, check this out.) .
     - Let's look at the input parameters for the OpenCV function HoughLinesP that we will use to find lines in the image. You will call it like this:
     - lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]), min_line_length, max_lin_gap)
     - In this case, we are operating on the image masked_edges (the output from Canny) and the output from HoughLinesP will be lines, which will simply be an array containing the endpoints (x1, y1, x2, y2) of all line segments detected by the transform operation. The other parameters define just what kind of line segments we're looking for.
     - First off, rho and theta are the distance and angular resolution of our grid in Hough space. Remember that, in Hough space, we have a grid laid out along the (Θ, ρ) axis. You need to specify rho in units of pixels and theta in units of radians.
     - So, what are reasonable values? Well, rho takes a minimum value of 1, and a reasonable starting place for theta is 1 degree (pi/180 in radians). Scale these values up to be more flexible in your definition of what constitutes a line.
     - The threshold parameter specifies the minimum number of votes (intersections in a given grid cell) a candidate line needs to have to make it into the output. The empty np.array([]) is just a placeholder, no need to change it. min_line_length is the minimum length of a line (in pixels) that you will accept in the output, and max_line_gap is the maximum distance (again, in pixels) between segments that you will allow to be connected into a single line. You can then iterate through your output lines and draw them onto the image to see what you got!

***** Code
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
# Do relevant imports
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import cv2

# Read in and grayscale the image
image_path = "/home/isaac/Dropbox/udacity/data/exit-ramp.jpg"
image = mpimg.imread(image_path)
gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)

# Define a kernel size and apply Gaussian smoothing
kernel_size = 5
blur_gray = cv2.GaussianBlur(gray,(kernel_size, kernel_size),0)

# Define our parameters for Canny and apply
low_threshold = 50
high_threshold = 150
masked_edges = cv2.Canny(blur_gray, low_threshold, high_threshold)

# Define the Hough transform parameters
# Make a blank the same size as our image to draw on
rho = 1
theta = np.pi/180
threshold = 1
min_line_length = 10
max_line_gap = 1
line_image = np.copy(image)*0 #creating a blank to draw lines on

# Run Hough on edge detected image
lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]),
                            min_line_length, max_line_gap)

# Iterate over the output "lines" and draw lines on the blank
for line in lines:
    for x1,y1,x2,y2 in line:
        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)

# Create a "color" binary image to combine with line image
color_edges = np.dstack((masked_edges, masked_edges, masked_edges)) 

# Draw the lines on the edge image
combo = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0) 
plt.imshow(combo)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[72]:
: <matplotlib.image.AxesImage at 0x7fc058f9d208>
[[file:./obipy-resources/31547aSj.png]]
:END:

***** Synthesis
      - As you can see I've detected lots of line segments! Your job, in the next exercise, is to figure out which parameters do the best job of optimizing the detection of the lane lines. Then, you'll want to apply a region of interest mask to filter out detected line segments in other areas of the image. Earlier in this lesson you used a triangular region mask, but this time you'll get a chance to use a quadrilateral region mask using the cv2.fillPoly() function (keep in mind though, you could use this same method to mask an arbitrarily complex polygon region). When you're finished you'll be ready to apply the skills you've learned to do the project at the end of this lesson.

**** Review

***** Code
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
rho = 1
theta = np.pi / 180
threshold = 1
min_line_length = 10
max_line_gap = 1

line_image = np.copy(image) * 0

lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)

for line in lines:
    for x1, y1, x2, y2 in line:
        cv2.line(line_image, (x1, y1), (x2, y2), (255,0,0), 10)
#plt.imshow(line_image, cmap="gray")
color_edges = np.dstack((edges, edges, edges))
combo = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0)
plt.imshow(combo)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[114]:
: <matplotlib.image.AxesImage at 0x7fc05876f630>
[[file:./obipy-resources/305mXx.png]]
:END:
     
***** TODO [[https://alyssaq.github.io/2014/understanding-hough-transform/][Understanding Hough Transform with Python]]
*** Quiz Hough Transform
    - Now it's your turn to play with the Hough Transform on an edge-detected image. You'll start with the image on the left below. If you "Test Run" the quiz, you'll get output that looks like the center image. Your job is to modify the parameters for the Hough Transform and impose a region of interest mask to get output that looks like the image on the right. In the code, I've given you a framework for defining a quadrilateral region of interest mask.

**** Review
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import cv2
%matplotlib inline

image_to_show = []

img_path = "/home/isaac/Dropbox/udacity/data/exit-ramp.jpg"
# TODO: Load Images and Convert to Gray
image = mpimg.imread(img_path)
image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
image_to_show.append(image_gray)

# TODO: Gaussian Smoothing
kernel_size = 5
image_blur_gray = cv2.GaussianBlur(image_gray, (kernel_size, kernel_size), 0)
image_to_show.append(image_blur_gray)

# TODO: Canny Edge
low_threshold = 50
high_threshold = 200
image_edges = cv2.Canny(image_blur_gray, low_threshold, high_threshold)
image_to_show.append(image_edges)

# TODO: Create a masked edges image
image_mask = np.zeros_like(image_edges)
ignore_mask_color = 255
imshape = image.shape
vertices = np.array([[(0, imshape[0]), (450, 290), (490, 290), (imshape[1], imshape[0])]], dtype=np.int32)
cv2.fillPoly(image_mask, vertices, ignore_mask_color)
image_to_show.append(image_mask)

# TODO: Combine mask with edges
masked_edges = cv2.bitwise_and(image_edges, image_mask)
image_to_show.append(masked_edges)

# TODO: Hough Transformation
rho = 1
theta = np.pi / 180
threshold = 1
min_line_length = 5
max_line_gap = 20
line_image = np.copy(image) * 0
lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)

for line in lines:
    for x1, y1, x2, y2 in line:
        cv2.line(line_image, (x1, y1), (x2, y2),(255, 0, 0), 10)
color_edges = np.dstack((edges, edges, edges))
line_edges = cv2.addWeighted(color_edges, 0.8, line_image, 1, 0)
image_to_show.append(line_edges)

plt.imshow(image_to_show[-1], cmap="gray");
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[156]:
[[file:./obipy-resources/305aca.png]]
:END:

*** Canny Edge

 #+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
fig_ce = plt.figure(figsize=(15,6))
img_dict_ce = {}
my_ax_ce = list()
i = 1
for (key, img) in img_dict_grey_ga.items():
    my_ax_ce.append(fig_ce.add_subplot(2,3,i))
    img_dict_ce[key] = canny(img, 100, 200)
    plt.imshow(img_dict_ce[key], cmap="Greys_r")
    plt.title(key)
    i += 1
plt.tight_layout()
 #+END_SRC

*** Mask Images
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
fig_mi = plt.figure(figsize=(15,6))
img_dict_mi = {}
my_ax_mi = list()
i = 1
vertices = np.array([[(0, image.shape[0]), (465, 320), (475, 320), (image.shape[1], image.shape[0])]], 
                    dtype=np.int32)

for (key, img) in img_dict_ce.items():
    my_ax_mi.append(fig_mi.add_subplot(2,3,i))
    img_dict_mi[key] = region_of_interest(img, vertices)
    plt.imshow(img_dict_mi[key], cmap="Greys_r")
    plt.title(key)
    i += 1
plt.tight_layout()
#+END_SRC

*** Hough Transformation
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
fig_ht = plt.figure(figsize=(15,6))
img_dict_ht = {}
my_ax_ht = list()
i = 1
vertices = np.array([[(0, image.shape[0]), (450, 320), (475, 320), (image.shape[1], image.shape[0])]], 
                    dtype=np.int32)

for (key, img) in img_dict_mi.items():
    my_ax_ht.append(fig_ht.add_subplot(2,3,i))
    img_dict_ht[key] = hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap)
    plt.imshow(img_dict_ht[key], cmap="Greys_r")
    plt.title(key)
    plt.suptitle('Hough Transformation', fontsize=20)
    i += 1
#+END_SRC

*** Combine Images
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
fig_final = plt.figure(figsize=(15,6))
img_dict_final = {}
my_ax_final = list()
i = 1

for (key, img) in img_dict_ht.items():
    my_ax_final.append(fig_final.add_subplot(2,3,i))
    img_dict_final[key] = hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap)
    plt.imshow(img_dict_final[key], cmap="Greys_r")
    plt.title(key)
    plt.suptitle('Hough Transformation', fontsize=20)
    i += 1
#+END_SRC

** Neural Networks

*** Forward Propagation

**** Basic Units
     - Data are fed into a network of interconnected nodes. These individual nodes are called perceptrons, or artificial neurons, and they are the basic unit of a neural network.
     - Each one looks at input data and decides how to categorize that data.
     - These categories then combine to form a decision

**** Weights
     - Well, when we initialize a neural network, we don't know what information will be most important in making a decision. It's up to the neural network to learn for itself which data is most important and adjust how it considers that data. It does this with something called weights.
     - When input comes into a perceptron, it gets multiplied by a weight value that is assigned to this particular input
     - the network adjusts the weights based on any errors in categorization that results from the previous weights. This is called training the neural network
     - A higher weight means the neural network considers that input more important than other inputs, and lower weight means that the data is considered less important. An extreme example would be zero and it would have no affect on the output of the perceptron.
**** Summing the Input Data
     - Each input to a perceptron has an associated weight that represents its importance. These weights are determined during the learning process of a neural network, called training.
     - In the next step, the weighted input data are summed to produce a single value, that will help determine the final output - whether a student is accepted to a university or not
     - The perceptron applies these weights to the inputs and sums them in a process known as linear combination.
**** Activation Function
     - Finally, the result of the perceptron's summation is turned into an output signal! This is done by feeding the linear combination into an activation function.
     - Activation functions are functions that decide, given the inputs into the node, what should be the node's output? Because it's the activation function that decides the actual output, we often refer to the outputs of a layer as its "activations".
     - bias will move values in one direction or another
     - just like the weights, the bias can also be updated and changed by the neural network during training. 
**** Perceptron Formula
     - The below perceptron formula returns 1 if the input belongs to the accepted category or returns 0 if it doesn't
     - Then the neural network starts to learn! Initially, the weights and bias are assigned a random value, and then they are updated using a learning algorithm like gradient descent. The weights and biases change so that the next training example is more accurately categorized, and patterns in data are "learned" by the neural network.
[[/home/isaac/Dropbox/udacity/data/perceptron-equation-2.gif]]
**** Perceptrons as Logical Operators
     - Set the weights (weight1, weight2) and bias bias to the correct values that calculate AND operation as shown above.
**** Perceptron Algorithm
***** Code
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import numpy as np
import pandas as pd
np.random.seed(42)
data_path = "/home/isaac/Dropbox/udacity/data/intro-NN/data.csv"
data = pd.read_csv(data_path, names=["x1", "x2", "y"])
X = data[["x1", "x2"]]
y = data["y"]
stepFunction = lambda t: 1 if t >=0 else 0

W = [0.2, 0.3]
np.matmul(X, W)
# np.dot(W, X.iloc[0])
# def prediction(X, W, b):
#     stepFunction((np.matmul(X,W) + b)[0])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[99]:
: 0.13700130000000002
:END:
*** Error Functions
**** Log-loss Error Function
*** Discrete vs. Continuous
    - In the last few videos, we learned that continuous error functions are better than discrete error functions, when it comes to optimizing. For this, we need to switch from discrete to continuous predictions.
**** Quiz
     - The sigmoid function is defined as sigmoid(x) = 1/(1+e**(-x)), If the score is defined by 4x1 + 5x2 - 9 = score, then which of the following points has exactly a 50% probability of being blue or red?
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import numpy as np
# lambda doesn't need def
sigmoid = lambda x: 1/(1+np.exp(-x))
linear = lambda x,y: 4*x +5*y -9

print(sigmoid(linear(1,1)))
print(sigmoid(linear(2,4)))
print(sigmoid(linear(5,5)))
print(sigmoid(linear(-4,5)))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[13]:
:END:
*** Multi-Class Classification and Softmax
**** The Softmax Function
     - exponential function turns every number into positive number
     - Linear function scores: Z1, Z2,..., Zn
     - P(class i) = e ** (Zi) / (e ** Z1+e**Z2+...+e**Zn))
     - Softmax for n=2 values the same as the sigmoid function
**** Coding Softmax
#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
import numpy as np

def softmax(L):
	all_elements = [np.exp(i) for i in L]
	results = [e/np.sum(all_elements) for e in all_elements]
	return results

softmax([1,2,3,4,5])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[17]:
#+BEGIN_EXAMPLE
  [0.011656230956039607,
  0.031684920796124269,
  0.086128544436268703,
  0.23412165725273662,
  0.63640864655883078]
#+END_EXAMPLE
:END:
*** One-Hot Encoding
[[./resources/one-hot-encoding.png]]
